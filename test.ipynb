{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.         0.         0.03846154 0.01923077 0.         0.\n",
      " 0.07692308 0.         0.         0.         0.19230769 0.\n",
      " 0.         0.         0.5        0.        ]\n",
      "[0.         0.06140351 0.18421053 0.18421053 0.         0.\n",
      " 0.18421053 0.         0.         0.23684211 0.36842105 0.\n",
      " 0.         0.34210526 0.68421053 0.        ]\n",
      "[0.14393939 0.11363636 0.22727273 0.22727273 0.17424242 0.\n",
      " 0.22727273 0.         0.20454545 0.40909091 0.45454545 0.\n",
      " 0.         0.56818182 0.72727273 0.        ]\n",
      "[0.75       0.54166667 0.33333333 0.33333333 0.75       0.\n",
      " 0.33333333 0.         0.75       0.75       0.66666667 0.\n",
      " 0.         0.83333333 0.91666667 0.        ]\n",
      "[0.7804878  0.65853658 0.53658537 0.53658537 0.7804878  0.\n",
      " 0.41463415 0.         0.7804878  0.7804878  0.70731707 0.\n",
      " 0.         0.85365854 0.92682927 0.        ]\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 7.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 8.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 9.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 10.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 11.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 12.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 13.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 14.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 15.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 16.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 17.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 18.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 19.\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "Policy-Iteration converged as step 20.\n",
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "# 4*4的网格，有16个格子（状态），分别用0-15表示。eon=16\n",
    "eon = env.observation_space.n\n",
    "# 4个动作——上下左右，分别用0-3表示。ean=4\n",
    "ean = env.action_space.n\n",
    "\n",
    "\n",
    "# 计算值函数\n",
    "def compute_value_function(policy, gamma=1.0):\n",
    "    # 初始化V表\n",
    "    value_table = np.zeros(eon)\n",
    "    # 收敛判断阈值\n",
    "    threshold = 1e-10\n",
    "    # 循环直到收敛\n",
    "    while True:\n",
    "        # 初始化更新后的V表（旧表复制过来）\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        # 计算每个状态从策略中得到的动作，然后计算值函数\n",
    "        # 遍历每个状态\n",
    "        for state in range(eon):\n",
    "            # 根据策略取动作\n",
    "            action = policy[state]\n",
    "            # 更新该状态的V值（公式）\n",
    "            value_table[state] = sum([trans_prob*(reward+gamma*updated_value_table[next_state])\n",
    "                                      for trans_prob, next_state, reward, done in env.P[state][action]])\n",
    "        # 收敛判断\n",
    "        if (np.sum((np.fabs(updated_value_table-value_table))) <= threshold):\n",
    "            break\n",
    "    # 返回V表\n",
    "    return value_table\n",
    "\n",
    "\n",
    "# 策略选取（同上）\n",
    "def extract_policy(value_table, gamma=1.0):\n",
    "    # 初始化存储策略的数组\n",
    "    policy = np.zeros(eon)\n",
    "    # 对每个状态构建Q表，并在该状态下对每个行为计算Q值，\n",
    "    for state in range(eon):\n",
    "        # 初始化Q表\n",
    "        Q_table = np.zeros(ean)\n",
    "        # 对每个动作计算\n",
    "        for action in range(ean):\n",
    "            # 同上\n",
    "            for next_sr in env.P[state][action]:\n",
    "                trans_prob, next_state, reward, done = next_sr\n",
    "                # 更新Q表，即更新动作对应的Q值（4个动作分别由0-3表示）\n",
    "                Q_table[action] += (trans_prob *\n",
    "                                    (reward+gamma*value_table[next_state]))\n",
    "        # 当前状态下，选取使Q值最大的那个策略\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    # 返回策略\n",
    "    return policy\n",
    "\n",
    "\n",
    "# 策略迭代\n",
    "def policy_iteration(env, gamma=1.0):\n",
    "    # 初始化随机策略，下句代码即为初始策略全为0（向左走）\n",
    "    random_policy = np.zeros(eon)\n",
    "    # 设置迭代次数\n",
    "    no_of_iterations = 20\n",
    "    # 开始迭代\n",
    "    for i in range(no_of_iterations):\n",
    "        # 计算新的值函数\n",
    "        new_value_function = compute_value_function(random_policy, gamma)\n",
    "        print(new_value_function)\n",
    "        # 得到新的策略\n",
    "        new_policy = extract_policy(new_value_function, gamma)\n",
    "        # 判断迭代终止条件（策略不变时）\n",
    "        if (np.all(random_policy == new_policy)):\n",
    "            print('Policy-Iteration converged as step %d.' % (i+1))\n",
    "            #break\n",
    "        # 新的策略为下一次的执行策略\n",
    "        random_policy = new_policy\n",
    "    # 返回新的策略\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "# 输出最优策略\n",
    "print(policy_iteration(env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linqian/anaconda3/envs/diffuser/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import diffuser.environments\n",
    "env=gym.make('mycliffwalking-medium-replay-v0')\n",
    "d = env.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for l in range(200):\n",
    "    import numpy as np\n",
    "    action = d['actions'][l]\n",
    "    observation = d['observations'][l]\n",
    "    dire = ['U', 'R', 'D', 'L']\n",
    "    print(\"action:   \", dire[np.argmax(action)], np.max(action))\n",
    "    for k, traj in enumerate(observation):\n",
    "        o = np.zeros((4,12))\n",
    "        for i in range(8):\n",
    "            pos = np.argmax(traj[i, :])\n",
    "            o[ np.unravel_index(pos, o.shape)] = i+1\n",
    "        print(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('diffuser')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb2c24f9eb8ecfc8bd9e5144333039883759be71dc97e67730274212aed26e26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
